# Deep Learning 

学习方式
1. 监督学习（Supervised learning），监督学习指的是用打好标签的数据训练预测新数据的类型或值。
2. 无监督学习（Unsupervised learning），无监督学习是在数据没有标签的情况下做数据挖掘, 无监督学习主要体现在聚类。简单来说是将数据根据不同的特征在没有标签的情况下进行分类。无监督学习的典型方法有k-聚类及主成分分析等。
3. 半监督学习（Semi-Supervised learning），半监督学习根据字面意思可以理解为监督学习和无监督学习的混合使用。事实上是学习过程中有标签数据和无标签数据相互混合使用。一般情况下无标签数据比有标签数据量要多得多。
4. 强化学习（Reinforcement learning），强化学习是通过与环境的交互获得奖励, 并通过奖励的高低来判断动作的好坏进而训练模型的方法，可以得到一个延迟的反馈，并且只有提示你是离答案越来越近还是越来越远。
5. 弱监督学习，弱监督通常分为三种类型：不完全监督（半监督学习）、不确切监督、不准确监督。不确切监督：训练的数据有一个弱标签，希望通过深度学习，得到一个强标签。比如说，知道一张图片是一只猫，通过训练知道猫在那里，将猫和背景分离开来。不准确监督：有些标签是错误的，不准确的。


一些小的总结：
深度学习的发展历程
- 1943年, 心理学家McCulloch和数理逻辑学家Pitts提出了神经元的第1个数学模型———MP模型（以他们两的名字命名. 它大致模拟了人类神经元的工作原理, 但需要手动设置权重, 十分不便。MP模型具有开创意义, 为后来的研究工作提供了依据.
- 1958年, Rosenblatt教授提出了感知机模型 (perceptron) , Rosenblatt在MP模型的基础之上增加了学习功能, 提出了单层感知器模型, 第一次把神经网络的研究付诸实践[2, 3]. 尽管相比MP模型, 该模型能更自动合理地设置权重, 但同样存在较大的局限, 难以展开更多的研究。
- Minsky教授于和Paper教授于1969年证明了感知机模型只能解决线性可分问题, 不能够处理线性不可分问题，并且否定了多层神经网络训练的可能性, 甚至提出了“基于感知机的研究终会失败”的观点, 此后十多年的时间内, 神经网络领域的研究基本处于停滞状态。
- 20世纪80年代, 计算机飞速发展, 计算能力相较以前也有了质的飞跃。直至1986年, Rumelhart等人 在Nature上发表文章,提出了一种按误差逆传播算法训练的多层前馈网络—反向传播网络 (Back PropagationNetwork, BP网络) , 解决了原来一些单层感知器所不能解决的问题. BP算法的提出不仅有力地回击了Minsky教授等人的观点, 更引领了神经网络研究的第二次高潮。随后, 玻尔兹曼机、卷积神经网络、循环神经网络等神经网络结构模型均在这一时期得到了较好的发展。
- 由于在20世纪90年代, 各种浅层机器学习模型相继被提出, 较经典的如支持向量机, 而且当增加神经网络的层数时传统的BP网络会遇到局部最优、过拟合及梯度扩散等问题, 这些使得深度模型的研究被搁置.
- 1990 年, LeCun等提出了现代 CNN 框架的原始版本, 之后又对其进行了改进, 于1998年提出了基于梯度学习的CNN模型——LeNet-5, 并将其成功应用于手写数字字符的识别中, 1998年的LeNet.最早提出了卷积神经网络，并用于手写数字识别.只是由于当时缺乏大规模的训练数据, 计算机的计算能力也有限, 所以LeNet在解决复杂问题 (例如大规模的图像和视频分类问题) 时, 效果并不好
- 2006年, 机器学习领域泰斗Hinton及其团队在Science上发表了关于神经网络理念突破性的文章, 首次提出了深度学习的概念, 并指明可以通过逐层初始化来解决深度神经网络在训练上的难题。该理论的提出再次激起了神经网络领域研究的浪潮。Hinton教授解决了BP神经网络算法梯度消失的问题, 深度学习的思想再次回到了大众的视野之中, 也正因为如此, 2006年被称为是深度学习发展的元年。
- 2011年, 吴恩达领导Google科学家们用16000台电脑成功模拟了一个人脑神经网络;
- 2012年, Hinton教授带领团队参加ImageNet图像识别比赛。在比赛中, Hinton团队所使用的深度学习算法一举夺魁, 其性能达到了碾压第二名SVM算法的效果, 自此深度学习的算法思想受到了业界研究者的广泛关注。深度学习的算法也渐渐在许多领域代替了传统的统计学机器学习方法, 成为人工智能中最热门的研究领域
- 2014年, 2014 年出现了两个很有影响力的卷积神经网络模型——依旧致力于加深模型层数的 VGGNet 和在模型结构上进行优化的 Inception Net深度学习模型Top-5在ImageNet 2014计算机识别竞赛上拔得头筹, 同年, 腾讯和京东也分别成立了自己的深度学习研究室。
- 2014年, 生成对抗网络的提出是深度学习的又一突破性进展, 将生成模型和判别模型紧密联系起来。(这里生成对抗网络的最原始形态，叫做 GAN，随后进化出来 diffusion 等模型，这些模型使得)
- 2016年, AlphaGo击败围棋世界冠军李在石, 同年9月, 中国科学院计算技术研究所发布“寒武纪1A”深度神经元网络处理器。这一切都显著地表明了一个事实:深度学习正在有条不紊地发展着, 其影响力不断扩大。
2012年, Hinton教授带领团队参加ImageNet图像识别比赛。在比赛中, Hinton团队所使用的深度学习算法一举夺魁, 其性能达到了碾压第二名SVM算法的效果, 自此深度学习的算法思想受到了业界研究者的广泛关注。深度学习的算法也渐渐在许多领域代替了传统的统计学机器学习方法, 成为人工智能中最热门的研究领域

上述发展历程中，有些看原始 paper 会很晦涩难懂，不如看一些其他人写的总结，比如：
- svm 算法的总结，目前为止我看到的比较好的是：https://zhuanlan.zhihu.com/p/29862011
- perceptron，多层感知机总结：https://zhuanlan.zhihu.com/p/190184033
- 深度学习一些基本的操作的细节，从细节出发，知道什么是卷积，什么是全连接，才能理解为什么网络这么设计：
    - 卷积：https://zhuanlan.zhihu.com/p/76606892
    - 全连接：https://blog.csdn.net/weixin_45829462/article/details/106548749
- 几个非常经典的，奠定了一些深度学习模型设计的几个文章：
    - 多层感知机 mlp 总结：https://zhuanlan.zhihu.com/p/63184325
    - vggnet paper：https://arxiv.org/abs/1409.1556.pdf
    - vggnet 总结：https://blog.csdn.net/Alter__/article/details/117444720
    - alexnet paper: https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
    - alexnet 总结：https://blog.csdn.net/hongbin_xu/article/details/80271291
    - gru paper：https://arxiv.org/pdf/1412.3555.pdf
    - gru 总结：https://zhuanlan.zhihu.com/p/32481747
    - lstm paper：https://arxiv.org/abs/1402.1128.pdf
    - lstm 总结：https://zhuanlan.zhihu.com/p/38063427
    - transformer paper：https://arxiv.org/abs/1706.03762
    - transformer 总结：https://zhuanlan.zhihu.com/p/338817680
