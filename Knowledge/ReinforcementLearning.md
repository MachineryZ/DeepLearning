### 动态规划 (Dynamic Programming, DP)

- **时间：1950年代**
- **方法简介：** 利用贝尔曼方程，包含策略迭代和价值迭代两种主要方法。
- **特点：** 需要已知环境的状态转移模型，适用于小规模、完全可观测的马尔可夫决策过程（MDP）。

### 蒙特卡洛方法 (Monte Carlo Methods)

- **时间：1950年代**
- **方法简介：** 通过模拟得到样本估计值函数，进行策略优化。包括第一访问蒙特卡洛和每步蒙特卡洛。
- **特点：** 不需要已知环境模型，适用于长期累计奖励的估计。

### 时间差分学习 (Temporal Difference Learning, TD)

- **时间：1980年代**
- **方法简介：** 结合蒙特卡洛方法和动态规划，通过TD误差更新值函数。TD(0)是最基本的方法。
- **特点：** 适用于在线学习，不需要完整的轨迹数据。

### SARSA (State-Action-Reward-State-Action)

- **时间：1980年代**
- **方法简介：** 一种基于时间差分的在线学习算法，更新动作值函数Q。
- **特点：** 是一种在线、策略内更新的方法，使用当前策略生成的动作进行更新。

### Q-learning

- **时间：1989年**
- **方法简介：** 使用离线更新的方法，通过最大化未来奖励更新Q值。
- **特点：** 是一种无模型、离线、策略外更新的方法，对探索和利用有良好平衡。

### 策略梯度方法 (Policy Gradient Methods)

- **时间：2000年代**
- **方法简介：** 直接优化策略函数π，REINFORCE是其中一种基础算法。
- **特点：** 适用于处理连续动作空间，通过梯度上升优化策略。

### Actor-Critic 方法

- **时间：2000年代**
- **方法简介：** 结合策略梯度和价值函数的方法，使用actor更新策略，critic更新值函数。
- **特点：** 改善了策略梯度方法的高方差问题，提高了学习效率。

### 函数逼近 (Function Approximation)

- **时间：1990年代**
- **方法简介：** 使用神经网络、线性回归等方法对值函数进行逼近，解决大规模状态空间的问题。
- **特点：** 使强化学习应用于高维空间成为可能。

### 深度Q网络 (Deep Q-Network, DQN)

- **时间：2013年**
- **方法简介：** 结合深度学习和Q-learning，使用卷积神经网络对Q值进行逼近。
- **特点：** 通过经验回放和固定目标网络解决深度学习中的不稳定问题，在高维输入下表现出色。

### 双重深度Q网络 (Double DQN)

- **时间：2015年**
- **方法简介：** 使用双网络解决Q-learning中的过高估计问题。
- **特点：** 提高了DQN的稳定性和性能。

### 优先经验回放 (Prioritized Experience Replay)

- **时间：2015年**
- **方法简介：** 根据经验的TD误差大小来优先选择经验进行训练。
- **特点：** 提高了样本效率，加速学习过程。

### 深度确定性策略梯度 (Deep Deterministic Policy Gradient, DDPG)

- **时间：2015年**
- **方法简介：** 结合DQN和Actor-Critic方法，用于连续动作空间的深度强化学习。
- **特点：** 能处理高维的连续动作空间，通过策略目标网络和经验回放稳定训练过程。

### 双重深度确定性策略梯度 (Twin Delayed DDPG, TD3)

- **时间：2018年**
- **方法简介：** 通过引入双重网络和延迟更新策略解决DDPG中的高估计问题。
- **特点：** 提高了DDPG的稳定性和性能。

### 软行为者-评论者 (Soft Actor-Critic, SAC)

- **时间：2018年**
- **方法简介：** 引入熵正则化项，鼓励策略探索，结合最大熵强化学习和Actor-Critic方法。
- **特点：** 提高了策略的探索能力，在连续动作空间任务中表现出色。

### 近端策略优化 (Proximal Policy Optimization, PPO)

- **时间：2017年**
- **方法简介：** 使用裁剪的目标函数限制策略更新的幅度，保证训练过程的稳定性。
- **特点：** 易于实现，训练稳定性和高效性俱佳，被广泛应用。

### 优势行为者-评论者 (Advantage Actor-Critic, A2C/A3C)

- **时间：2016年**
- **方法简介：** 使用多个并行环境进行训练，优势函数用于减小方差。
- **特点：** 提高了训练速度和效果，在分布式计算环境下表现优异。

### 异步优势行为者-评论者 (Asynchronous Advantage Actor-Critic, A3C)

- **时间：2016年**
- **方法简介：** 使用多线程并行环境进行训练，异步更新权重。
- **特点：** 显著加快了训练速度，并提高了样本效率。

### TRPO (Trust Region Policy Optimization)

- **时间：2015年**
- **方法简介：** 使用信任区域限制策略更新，确保策略更新时不会偏离太多。
- **特点：** 提高了策略更新的稳定性和性能。

### ACER (Actor-Critic with Experience Replay)

- **时间：2016年**
- **方法简介：** 结合经验回放和Actor-Critic方法，通过重要性采样修正偏差。
- **特点：** 在稳定性和样本效率之间取得良好平衡。

### IMPALA (Importance Weighted Actor-Learner Architectures)

- **时间：2018年**
- **方法简介：** 使用分布式框架并行训练多个agent，使用重要性采样修正偏差。
- **特点：** 提高了样本效率和训练速度，适用于大规模分布式环境。

### GAIL (Generative Adversarial Imitation Learning)

- **时间：2016年**
- **方法简介：** 使用生成对抗网络(GAN)进行模仿学习，通过模仿专家策略来学习。
- **特点：** 能有效进行无模型模仿学习，提高了策略学习的效率。

### Rainbow

- **时间：2017年**
- **方法简介：** 结合多个DQN的改进，包括Double DQN, Prioritized Experience Replay, Dueling DQN, Noisy Nets等。
- **特点：** 提供了更稳健和高效的深度Q学习方法。

### HER (Hindsight Experience Replay)

- **时间：2017年**
- **方法简介：** 在目标导向任务中，利用失败的经验，通过重新定义目标进行训练。
- **特点：** 提高了稀疏奖励环境中的学习效率。

### AWR (Advantage Weighted Regression)

- **时间：2019年**
- **方法简介：** 使用优势函数加权回归方法，结合离线和在线强化学习。
- **特点：** 提高了策略更新的稳定性和效率。

### MPO (Maximum a Posteriori Policy Optimization)

- **时间：2018年**
- **方法简介：** 通过最大后验估计方法进行策略优化，平衡探索和利用。
- **特点：** 提高了策略更新的稳定性和性能。

### CURL (Contrastive Unsupervised Representation Learning)

- **时间：2020年**
- **方法简介：** 使用对比学习方法进行无监督特征学习，提高状态表示的质量。
- **特点：** 提高了数据效率和泛化能力。

### Dreamer

- **时间：2020年**
- **方法简介：** 基于世界模型的方法，通过学习环境的动力学模型进行规划。
- **特点：** 提高了复杂环境中的学习效率。

### MuZero

- **时间：2019年**
- **方法简介：** 结合AlphaZero和世界模型，不需要环境模型的情况下进行高效学习。
- **特点：** 适用于多种任务，包括棋类游戏和Atari游戏。

### Meta-RL (Meta-Reinforcement Learning)

- **时间：2017年起**
- **方法简介：** 通过元学习方法提高在新任务中的学习速度和效果。
- **特点：** 增强了算法在不同任务中的泛化能力。

### TQC (Truncated Quantile Critics)

- **时间：2020年**
- **方法简介：** 使用截断的分位数回归方法，解决分布式强化学习中的稳定性问题。
- **特点：** 提高了分布式强化学习算法的性能。

### PG-MARL (Policy Gradient Methods for Multi-Agent RL)

- **时间：2000年代起**
- **方法简介：** 适用于多智能体环境的策略梯度方法，如MADDPG。
- **特点：** 解决了多智能体环境中的策略优化问题。

### H-DQN (Hierarchical Deep Q-Network)

- **时间：2016年**
- **方法简介：** 通过分层架构处理复杂任务，高层决定子任务，低层完成具体操作。
- **特点：** 提高了复杂任务中的学习效率。



------

TD learning 是用于学习估计环境中 action function 或者 value function 的方法，它结合了动态规划和蒙特卡洛方法的优点，并且能够在不需要完整的环境模型的情况下学习。在 td learning 中，agent从与environment的交互中学习，并且在每一步都更新其估计值函数，这种方法不需要等到最终的奖励。而是使用当前状态和后续状态的估计值的差异来更新 value function，因此得名 td learning







----



-------



-----

