大模型 RAG 是什么

什么是 RAG retrieval augmented generation 检索增强生成是指对大型语言模型输出进行优化，使其能够在生成响应之前引用训练数据来源之外的权威知识库，大型语言模型 LLM 用海量数据进行训练，使用数十亿个参数为回答问题、翻译语言和完成句子等任务生成原始输出。在 LLM 本就强大的功能基础上，RAG将其拓展为能访问特定领域或组织的内部知识库，所有这些都无需重新训练模型。这是一种经济高效的改进 LLM 输出的方法，让他在各种情境下都能保持相关性、准确性和实用性

通用大模型无法解决实际业务需求，主要有以下几方面的原因：

1. 知识的局限性：模型自身的知识完全源于它的训练数据，而现有的主流大模型 chatgpt的训练机基本都是构建于网络公开的数据，对于一些实时性的、非公开的或离线数据是无法获取到的，这部分知识也就无从具备
2. 幻觉问题，所有的ai模型的底层原理都是基于数学概率，其模型输出实质上是一些列的数值计算，大模型也不例外，所以他有时候会一本征集的胡说八道，尤其是在大模型自身不具备某一方面的知识或不擅长的场景，而这种幻觉问题的区分是比较困难的，是因为他要求使用者自身具备相应领域的知识
3. 数据安全性，对于企业来说，数据安全至关重要，没有企业愿意承担数据泄露的风险，将自身私域数据上传第三方平台进行训练。这也导致完全依赖通用大模型自身能力的应用方案不得不在数据安全和效果方面进行取舍

RAG 是解决上述问题的一套有效方案：

RAG 检索增强生成 = 检索技术 + LLM 提示。例如，我们向 LLM 提问一个问题，RAG从各种数据源检索相关的信息，并将检索到的信息和问题注入到 LLM 提示中，LLM 最后给出答案。



## **RAG架构**

RAG的架构如图中所示，简单来讲，RAG就是通过检索获取相关的知识并将其融入Prompt，让大模型能够参考相应的知识从而给出合理回答。因此，可以将RAG的核心理解为“检索+生成”，前者主要是利用向量数据库的高效存储和检索能力，召回目标知识；后者则是利用大模型和Prompt工程，将召回的知识合理利用，生成目标答案。

完整的RAG应用流程主要包含两个阶段：

- 数据准备阶段：数据提取——>文本分割——>向量化（embedding）——>数据入库
- 应用阶段：用户提问——>数据检索（召回）——>注入Prompt——>LLM生成答案

- **数据提取**
  - 数据加载：包括多格式数据加载、不同数据源获取等，根据数据自身情况，将数据处理为同一个范式。
  - 数据处理：包括数据过滤、压缩、格式化等。
  - 元数据获取：提取数据中关键信息，例如文件名、Title、时间等 。
- **文本分割**：
  文本分割主要考虑两个因素：1）embedding模型的Tokens限制情况；2）语义完整性对整体的检索效果的影响。一些常见的文本分割方式如下：
  - 句分割：以”句”的粒度进行切分，保留一个句子的完整语义。常见切分符包括：句号、感叹号、问号、换行符等。
  - 固定长度分割：根据embedding模型的token长度限制，将文本分割为固定长度（例如256/512个tokens），这种切分方式会损失很多语义信息，一般通过在头尾增加一定冗余量来缓解。
- **向量化（embedding）**：向量化是一个将文本数据转化为向量矩阵的过程，该过程会直接影响到后续检索的效果。目前常见的embedding模型如表中所示，这些embedding模型基本能满足大部分需求，但对于特殊场景（例如涉及一些罕见专有词或字等）或者想进一步优化效果，则可以选择开源Embedding模型微调或直接训练适合自己场景的Embedding模型。

























